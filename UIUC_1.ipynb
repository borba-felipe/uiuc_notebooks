{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UIUC_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b43c38a8862346fa8b089304006bf6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7db72f88e1b441bb632d8c0bd67709f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2831a385ac264558aeafb0bd2b89ce89",
              "IPY_MODEL_f6a745251eee40a7ad3f982fb6c15280"
            ]
          }
        },
        "a7db72f88e1b441bb632d8c0bd67709f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2831a385ac264558aeafb0bd2b89ce89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1da055e435834bbb9671cd77d1a58db8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_173bfda4b3e94790be8a21d51258f2ed"
          }
        },
        "f6a745251eee40a7ad3f982fb6c15280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4dc6fe9e6be443ac881eac0b0ed8d2a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [05:04&lt;00:00, 154kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_422d37d692894b3ba537275658cede87"
          }
        },
        "1da055e435834bbb9671cd77d1a58db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "173bfda4b3e94790be8a21d51258f2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4dc6fe9e6be443ac881eac0b0ed8d2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "422d37d692894b3ba537275658cede87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-VNVkd-EgwA"
      },
      "source": [
        "# DataSet: FMD\n",
        "# Treino da CNN e Validação do treino: base de dados dividida ao meio aleatória mente\n",
        "#                                      sendo 50% treino, 50% validação\n",
        "# A T E N Ç Ã O: Não se esqueça de alterar o nome dos arquivos que vão receber\n",
        "#                 os dados estatísticos\n",
        "xbase = 'UIUC'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMWfh3zDFDv-",
        "outputId": "6236271e-43a0-4424-98e2-6c6b2d9b23b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Montando o google drive contendo o DataSet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV-4F9OVOcyA"
      },
      "source": [
        "# Carregando bibliotecas do python e criando classes importantes para todo o código\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "from __future__ import print_function, division\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as python_multiprocessing \n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import os.path\n",
        "from PIL import Image\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import torch.utils.data as data\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "#import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch._six import int_classes as _int_classes\n",
        "import threading\n",
        "import itertools\n",
        "import warnings\n",
        "from torch._six import container_abcs, string_classes, int_classes\n",
        "import torch.multiprocessing as multiprocessing\n",
        "from torch._utils import ExceptionWrapper\n",
        "from torch._six import queue, string_classes\n",
        "from torch.utils.data._utils import signal_handling\n",
        "from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL\n",
        "from torch.utils.data._utils import ExceptionWrapper\n",
        "from torch.utils.data._utils import IS_WINDOWS\n",
        "from torch.utils.data import IterableDataset\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        " \n",
        "model_urls = {'resnet18':'https://download.pytorch.org/models/resnet18-5c106cde.pth'}\n",
        " \n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        " \n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                   padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        " \n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  __constants__=['downsample']\n",
        " \n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = norm_layer(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = norm_layer(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        " \n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "    \n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        " \n",
        "    return out\n",
        " \n",
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "  __constants__ = ['downsample']\n",
        " \n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "      width = int(planes * (base_width / 64.)) * groups\n",
        "      self.conv1 = conv1x1(inplanes, width)\n",
        "      self.bn1 = norm_layer(width)\n",
        "      self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "      self.bn2 = norm_layer(width)\n",
        "      self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "      self.bn3 = norm_layer(planes * self.expansion)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.downsample = downsample\n",
        "      self.stride = stride\n",
        " \n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        " \n",
        "    return out\n",
        " \n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64,\n",
        "               replace_stride_with_dilation=None, norm_layer=None):\n",
        "    super(ResNet, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    self._norm_layer = norm_layer\n",
        "    self.inplanes = 64\n",
        "    self.dilation = 1\n",
        "    if replace_stride_with_dilation is None:\n",
        "      replace_stride_with_dilation = [False, False, False]\n",
        "    self.groups = groups\n",
        "    self.base_width = width_per_group\n",
        "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = norm_layer(self.inplanes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "      elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      #valor1 = input(\"digite algo\")\n",
        "      #print(nn.init.constant_(m.weight, 1))\n",
        "      #valor2 = input(\"digite algo\")\n",
        "    if zero_init_residual:\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, Bottleneck):\n",
        "          nn.init.constant_(m.bn3.weight, 0)\n",
        "        elif isinstance(m, BasicBlock):\n",
        "          nn.init.constant_(m.bn2.weight, 0)\n",
        "  def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "    norm_layer = self._norm_layer\n",
        "    downsample = None\n",
        "    previous_dilation = self.dilation\n",
        "    if dilate:\n",
        "      self.dilation *= stride\n",
        "      stride = 1\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride), \n",
        "          norm_layer(planes * block.expansion),\n",
        "      )\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                        self.base_width, previous_dilation, norm_layer))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes, groups=self.groups, \n",
        "                          base_width=self.base_width, dilation=self.dilation,\n",
        "                          norm_layer = norm_layer))\n",
        "    return nn.Sequential(*layers)\n",
        " \n",
        "  def _forward_impl(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        " \n",
        "  def forward(self, x):\n",
        "    return self._forward_impl(x)\n",
        "  \n",
        "  def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "      state_dict = load_state_dict_from_url(model_urls['resnet18'],\n",
        "                                            progress=progress)\n",
        "      model.load_state_dict(state_dict)\n",
        "    return model\n",
        " \n",
        " \n",
        " \n",
        "class VisionDataset(data.Dataset):\n",
        "    _repr_indent = 4\n",
        " \n",
        "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
        "        if isinstance(root, torch._six.string_classes):\n",
        "            root = os.path.expanduser(root)\n",
        "        self.root = root\n",
        " \n",
        "        has_transforms = transforms is not None\n",
        "        has_separate_transform = transform is not None or target_transform is not None\n",
        "        if has_transforms and has_separate_transform:\n",
        "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
        "                             \"be passed as argument\")\n",
        " \n",
        "        # for backwards-compatibility\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        " \n",
        "        if has_separate_transform:\n",
        "            transforms = StandardTransform(transform, target_transform)\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        raise NotImplementedError\n",
        " \n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        " \n",
        "    def __repr__(self):\n",
        "        head = \"Dataset \" + self.__class__.__name__\n",
        "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
        "        if self.root is not None:\n",
        "            body.append(\"Root location: {}\".format(self.root))\n",
        "        body += self.extra_repr().splitlines()\n",
        "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
        "            body += [repr(self.transforms)]\n",
        "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
        "        return '\\n'.join(lines)\n",
        " \n",
        "    def _format_transform_repr(self, transform, head):\n",
        "        lines = transform.__repr__().splitlines()\n",
        "        return ([\"{}{}\".format(head, lines[0])] +\n",
        "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
        " \n",
        "    def extra_repr(self):\n",
        "        return \"\"\n",
        " \n",
        " \n",
        "class StandardTransform(object):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        " \n",
        "    def __call__(self, input, target):\n",
        "        if self.transform is not None:\n",
        "            input = self.transform(input)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return input, target\n",
        " \n",
        "    def _format_transform_repr(self, transform, head):\n",
        "        lines = transform.__repr__().splitlines()\n",
        "        return ([\"{}{}\".format(head, lines[0])] +\n",
        "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
        " \n",
        "    def __repr__(self):\n",
        "        body = [self.__class__.__name__]\n",
        "        if self.transform is not None:\n",
        "            body += self._format_transform_repr(self.transform,\n",
        "                                                \"Transform: \")\n",
        "        if self.target_transform is not None:\n",
        "            body += self._format_transform_repr(self.target_transform,\n",
        "                                                \"Target transform: \")\n",
        " \n",
        "        return '\\n'.join(body)\n",
        " \n",
        "def has_file_allowed_extension(filename, extensions):\n",
        "    return filename.lower().endswith(extensions)\n",
        " \n",
        "def is_image_file(filename):\n",
        "    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n",
        " \n",
        "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None, treino=None, valid=None):\n",
        "    dir = os.path.expanduser(dir)\n",
        "    images=[]\n",
        "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
        "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "    if extensions is not None:\n",
        "        def is_valid_file(x):\n",
        "            return has_file_allowed_extension(x, extensions)\n",
        "    for target in sorted(class_to_idx.keys()):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                if is_valid_file(path):\n",
        "                    item = (path, class_to_idx[target])\n",
        "                    images.append(item)\n",
        "    im_r=[]\n",
        "    xtimes=len(images)\n",
        "    im_r=random.sample(images,xtimes)\n",
        "    xxtimes = int(xtimes/2)\n",
        "    imagens_treino=im_r[xxtimes:]\n",
        "    imagens_valid=im_r[:xxtimes]\n",
        "    if treino:\n",
        "        f_images = imagens_treino\n",
        "    if valid:\n",
        "        f_images = imagens_valid\n",
        "    return f_images\n",
        " \n",
        "class DatasetFolder(VisionDataset):\n",
        "    def __init__(self, root, loader, extensions=None, transform=None,\n",
        "                 target_transform=None, is_valid_file=None, treino=None, valid=None):\n",
        "        super(DatasetFolder, self).__init__(root, transform=transform,\n",
        "                                            target_transform=target_transform)\n",
        "        classes, class_to_idx = self._find_classes(self.root)\n",
        "        self.treino = treino\n",
        "        self.valid = valid\n",
        "        samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file, self.treino, self.valid)\n",
        "\n",
        "        if len(samples) == 0:\n",
        "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
        "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
        " \n",
        "        self.loader = loader\n",
        "        self.extensions = extensions\n",
        " \n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.samples = samples\n",
        "        self.targets = [s[1] for s in samples]\n",
        " \n",
        "    def _find_classes(self, dir):\n",
        "        global xclasses\n",
        "        if sys.version_info >= (3, 5):\n",
        "          \n",
        "            # Faster and available in Python 3.5 and above\n",
        "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "        else:\n",
        "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "        classes.sort()\n",
        "        xclasses = classes\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        " \n",
        "        return sample, target\n",
        " \n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        " \n",
        "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        " \n",
        "def pil_loader(path):\n",
        "    \n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        " \n",
        " \n",
        "def accimage_loader(path):\n",
        "    import accimage\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except IOError:\n",
        "        \n",
        "        return pil_loader(path)\n",
        " \n",
        " \n",
        "def default_loader(path):\n",
        "    from torchvision import get_image_backend\n",
        "    if get_image_backend() == 'accimage':\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        " \n",
        " \n",
        "class ImageFolder(DatasetFolder):\n",
        "    \n",
        "    def __init__(self, root, transform=None, target_transform=None,\n",
        "                 loader=default_loader, is_valid_file=None, treino=None, valid=None):\n",
        "        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
        "                                          transform=transform,\n",
        "                                          target_transform=target_transform,\n",
        "                                          is_valid_file=is_valid_file, treino=treino, valid=valid)\n",
        "        self.imgs = self.samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71aI2CpSVTrg"
      },
      "source": [
        "# Aqui é definido como as amostras serão tratadas na CNN\n",
        "# Nesse momento é definido com a amostra a será utilizada como teste\n",
        "# E as amostras b, c e d serão utilizadas para validação\n",
        "data_transforms= {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "Acc_it=[]\n",
        "modelo=[]\n",
        "X_train=[]\n",
        "X_val=[]\n",
        "val=[]\n",
        "model_train=[]\n",
        "model_val=[]\n",
        "val_inputs=[]\n",
        "val_labels=[]\n",
        "train_inputs=[]\n",
        "train_labels=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLXx2KMFSF6f"
      },
      "source": [
        "xend_bd = '/content/gdrive/My Drive/Amostras/'+xbase\n",
        "image_datasets= {'train': ImageFolder(os.path.join(xend_bd),data_transforms['train'],treino=True),\n",
        "                     'val': ImageFolder(os.path.join(xend_bd),data_transforms['val'],valid=True)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUCXgWFqFpVf"
      },
      "source": [
        "# esta função treina o modelo de CNN\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    global best_acc, val_labels, val_inputs, train_labels, train_inputs\n",
        "    best_acc = 0.0\n",
        "    ntreino=0\n",
        "    nval=0\n",
        "    val_labels = {}\n",
        "    val_inputs = {}\n",
        "    train_labels = {}\n",
        "    train_inputs = {}\n",
        "    xval_labels={}\n",
        "    xval_inputs={}\n",
        "    xtrain_labels={}\n",
        "    xtrain_inputs={}\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Época {}/{}'.format(epoch, num_epochs - 1))\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            xi=0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        xtrain_labels[str(ntreino)]=labels\n",
        "                        xtrain_inputs[str(ntreino)]=inputs\n",
        "                        ntreino = ntreino + 1\n",
        "                    if phase == 'val':\n",
        "                        xval_labels[str(nval)]=labels\n",
        "                        xval_inputs[str(nval)]=inputs\n",
        "                        nval = nval + 1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_epoch = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                if len(xtrain_labels) != 0:\n",
        "                    train_labels = xtrain_labels\n",
        "                if len(xtrain_inputs) != 0:\n",
        "                    train_inputs = xtrain_inputs\n",
        "                if len(xval_labels) != 0:\n",
        "                    val_labels = xval_labels\n",
        "                if len(xval_inputs) != 0:\n",
        "                    val_inputs = xval_inputs\n",
        "            if phase == 'val':\n",
        "                print('Acurácia da CNN : {:4f}'.format(epoch_acc))\n",
        "                print('-' * 25) \n",
        "        ntreino=0\n",
        "        nval=0\n",
        "    print('Mélhor acurácia da CNN: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SNu_UgCzdTR",
        "outputId": "d098252f-c7e3-453c-c624-4915edb174aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b43c38a8862346fa8b089304006bf6e3",
            "a7db72f88e1b441bb632d8c0bd67709f",
            "2831a385ac264558aeafb0bd2b89ce89",
            "f6a745251eee40a7ad3f982fb6c15280",
            "1da055e435834bbb9671cd77d1a58db8",
            "173bfda4b3e94790be8a21d51258f2ed",
            "4dc6fe9e6be443ac881eac0b0ed8d2a8",
            "422d37d692894b3ba537275658cede87"
          ]
        }
      },
      "source": [
        "# Gerando o primeiro modelo de CNN: model_ft\n",
        "dataloaders= {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=4,shuffle=True, num_workers=4), \n",
        "              'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=4,shuffle=True, num_workers=4)}\n",
        "dataset_sizes = {'train': len(image_datasets['train']), 'val':len(image_datasets['val'])}\n",
        "class_names = image_datasets['train'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = ResNet.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model_ft = model_ft.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=25)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43c38a8862346fa8b089304006bf6e3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Época 0/24\n",
            "Acurácia da CNN : 0.758000\n",
            "-------------------------\n",
            "Época 1/24\n",
            "Acurácia da CNN : 0.834000\n",
            "-------------------------\n",
            "Época 2/24\n",
            "Acurácia da CNN : 0.934000\n",
            "-------------------------\n",
            "Época 3/24\n",
            "Acurácia da CNN : 0.910000\n",
            "-------------------------\n",
            "Época 4/24\n",
            "Acurácia da CNN : 0.898000\n",
            "-------------------------\n",
            "Época 5/24\n",
            "Acurácia da CNN : 0.950000\n",
            "-------------------------\n",
            "Época 6/24\n",
            "Acurácia da CNN : 0.954000\n",
            "-------------------------\n",
            "Época 7/24\n",
            "Acurácia da CNN : 0.978000\n",
            "-------------------------\n",
            "Época 8/24\n",
            "Acurácia da CNN : 0.978000\n",
            "-------------------------\n",
            "Época 9/24\n",
            "Acurácia da CNN : 0.984000\n",
            "-------------------------\n",
            "Época 10/24\n",
            "Acurácia da CNN : 0.986000\n",
            "-------------------------\n",
            "Época 11/24\n",
            "Acurácia da CNN : 0.978000\n",
            "-------------------------\n",
            "Época 12/24\n",
            "Acurácia da CNN : 0.986000\n",
            "-------------------------\n",
            "Época 13/24\n",
            "Acurácia da CNN : 0.996000\n",
            "-------------------------\n",
            "Época 14/24\n",
            "Acurácia da CNN : 0.984000\n",
            "-------------------------\n",
            "Época 15/24\n",
            "Acurácia da CNN : 0.986000\n",
            "-------------------------\n",
            "Época 16/24\n",
            "Acurácia da CNN : 0.984000\n",
            "-------------------------\n",
            "Época 17/24\n",
            "Acurácia da CNN : 0.988000\n",
            "-------------------------\n",
            "Época 18/24\n",
            "Acurácia da CNN : 0.990000\n",
            "-------------------------\n",
            "Época 19/24\n",
            "Acurácia da CNN : 0.988000\n",
            "-------------------------\n",
            "Época 20/24\n",
            "Acurácia da CNN : 0.984000\n",
            "-------------------------\n",
            "Época 21/24\n",
            "Acurácia da CNN : 0.992000\n",
            "-------------------------\n",
            "Época 22/24\n",
            "Acurácia da CNN : 0.984000\n",
            "-------------------------\n",
            "Época 23/24\n",
            "Acurácia da CNN : 0.988000\n",
            "-------------------------\n",
            "Época 24/24\n",
            "Acurácia da CNN : 0.992000\n",
            "-------------------------\n",
            "Mélhor acurácia da CNN: 0.996000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TftY2CGH0opy"
      },
      "source": [
        "# retirando a última camada da CNN \n",
        "model2=torch.nn.Sequential(*list(model_ft.children())[:-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A37AdQj07OX"
      },
      "source": [
        "# Obtendo os vetores de recursos\n",
        "# xxt : armazena as entradas para o treino do próximo classificador\n",
        "xvezes = len(train_inputs)\n",
        "xt = np.zeros(((xvezes * 4),512), dtype=np.float64)\n",
        "x=0\n",
        "rrr = len(xt)-1\n",
        "for p in range(rrr):\n",
        "    xt[p][0] = xt[p][0]-1\n",
        "for i in range(xvezes):\n",
        "    vetor = model2(train_inputs[str(i)])\n",
        "    xvetor = vetor.cpu().detach().numpy()\n",
        "    xv=list(xvetor.shape)\n",
        "    ny = (xv[0])-1\n",
        "    for y in range(ny):\n",
        "        for j in range(512):\n",
        "            xt[x][j]=xvetor[y][j]\n",
        "        x = x+1\n",
        "hp = len(xt)\n",
        "for i in range(hp):\n",
        "    a = xt[i][0]\n",
        "    b = np.float64(-1)\n",
        "    if a == b:\n",
        "      xhp = i\n",
        "      break\n",
        "xxt = np.zeros((xhp,512), dtype=np.float64)\n",
        "for w in range(xhp):\n",
        "    for q in range(512):\n",
        "        xxt[w][q] = xt[w][q]\n",
        "\n",
        "# xxvd : armazena as entradas para a validação do próximo classificador\n",
        "xvezes = len(val_inputs)\n",
        "xvd = np.zeros(((xvezes * 4),512), dtype=np.float64)\n",
        "x=0\n",
        "rrr = len(xvd)-1\n",
        "for p in range(rrr):\n",
        "    xvd[p][0] = xvd[p][0]-1\n",
        "for i in range(xvezes):\n",
        "    vetor = model2(val_inputs[str(i)])\n",
        "    xvetor = vetor.cpu().detach().numpy()\n",
        "    xv=list(xvetor.shape)\n",
        "    ny = (xv[0])-1\n",
        "    for y in range(ny):\n",
        "        for j in range(512):\n",
        "            xvd[x][j]=xvetor[y][j]\n",
        "        x = x+1\n",
        "hp = len(xvd)\n",
        "for i in range(hp):\n",
        "    a = xvd[i][0]\n",
        "    b = np.float64(-1)\n",
        "    if a == b:\n",
        "      yhp = i\n",
        "      break\n",
        "xxvd = np.zeros((yhp,512), dtype=np.float64)\n",
        "for w in range(yhp):\n",
        "    for q in range(512):\n",
        "        xxvd[w][q] = xvd[w][q]\n",
        "\n",
        "# xxlv: armazena as etiquetas para o treino do próximo classificador\n",
        "xvezes = len(val_labels)\n",
        "xlv = np.zeros((xvezes * 4), dtype=np.float64)\n",
        "rrr = len(xlv)-1\n",
        "for x in range(rrr):\n",
        "    xlv[x] = xlv[x]-1\n",
        "x=0\n",
        "for i in range(xvezes):\n",
        "    xvetor = val_labels[str(i)].cpu().detach().numpy()\n",
        "    yv=list(xvetor.shape)\n",
        "    ny = (yv[0])-1\n",
        "    for y in range(ny):\n",
        "        xlv[x]=xvetor[y]\n",
        "        x = x+1\n",
        "hp = len(xlv)\n",
        "for i in range(hp):\n",
        "    a = xlv[i]\n",
        "    b = np.float64(-1)\n",
        "    if a == b:\n",
        "      xhp = i\n",
        "      break\n",
        "xxlv = np.zeros(xhp, dtype=np.float64)\n",
        "for w in range(xhp):\n",
        "    xxlv[w] = xlv[w]\n",
        "\n",
        "# xxlt : armazena as etiquetas para a validação do próximo classificador\n",
        "xvezes = len(train_labels)\n",
        "xlt = np.zeros((xvezes * 4), dtype=np.float64)\n",
        "rrr = len(xlt)-1\n",
        "for x in range(rrr):\n",
        "    xlt[x] = xlt[x]-1\n",
        "x=0\n",
        "for i in range(xvezes):\n",
        "    xvetor = train_labels[str(i)].cpu().detach().numpy()\n",
        "    yv=list(xvetor.shape)\n",
        "    ny = (yv[0])-1\n",
        "    for y in range(ny):\n",
        "        xlt[x]=xvetor[y]\n",
        "        x = x+1\n",
        "hp = len(xlt)\n",
        "for i in range(hp):\n",
        "    a = xlt[i]\n",
        "    b = np.float64(-1)\n",
        "    if a == b:\n",
        "      yhp = i\n",
        "      break\n",
        "xxlt = np.zeros(yhp, dtype=np.float64)\n",
        "for w in range(yhp):\n",
        "    xxlt[w] = xlt[w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51fgoMHc1bx9"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "names = [\"KNN\", \"SVC Linear\", \"Processo Gaussiano\",\n",
        "         \"Árvores de Decisão\", \"Floresta Aleatória\", \"MLP\", \"AdaBoost\",\n",
        "         \"Naive Bayes\", \"Regressão Logística\", \"Regressão Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    LogisticRegression(random_state=1, max_iter=1500),\n",
        "    LinearRegression()]\n",
        "i = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNyA52Un_nlg",
        "outputId": "342604d4-9185-4f47-bf72-563d643c5b7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"----------------------------------------------------------------------------------\")\n",
        "print(\"Classificadores utilizando os vetores de recursos tratados no melhor modelo da CNN\")\n",
        "print(\"**********************************************************************************\")\n",
        "xscores = []\n",
        "# w_classif será utilizada nas matrizes de confusão, assim como names\n",
        "w_classif=[]\n",
        "for name, clf in zip(names, classifiers):\n",
        "        clf.fit(xxt, xxlt)\n",
        "        w_classif.append(clf)\n",
        "        score = clf.score(xxvd, xxlv)\n",
        "        if i > 1:\n",
        "            print(\"*************************************************\")\n",
        "        print(\" Classificador \"+str(i)+\" : \"+name)\n",
        "        print(\" Acurácia: \"+str(score))\n",
        "        print()\n",
        "        xscores.append(score)\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "Classificadores utilizando os vetores de recursos tratados no melhor modelo da CNN\n",
            "**********************************************************************************\n",
            " Classificador 1 : KNN\n",
            " Acurácia: 0.9893333333333333\n",
            "\n",
            "*************************************************\n",
            " Classificador 2 : SVC Linear\n",
            " Acurácia: 0.9973333333333333\n",
            "\n",
            "*************************************************\n",
            " Classificador 3 : Processo Gaussiano\n",
            " Acurácia: 0.042666666666666665\n",
            "\n",
            "*************************************************\n",
            " Classificador 4 : Árvores de Decisão\n",
            " Acurácia: 0.28\n",
            "\n",
            "*************************************************\n",
            " Classificador 5 : Floresta Aleatória\n",
            " Acurácia: 0.8026666666666666\n",
            "\n",
            "*************************************************\n",
            " Classificador 6 : MLP\n",
            " Acurácia: 0.9946666666666667\n",
            "\n",
            "*************************************************\n",
            " Classificador 7 : AdaBoost\n",
            " Acurácia: 0.18666666666666668\n",
            "\n",
            "*************************************************\n",
            " Classificador 8 : Naive Bayes\n",
            " Acurácia: 0.9173333333333333\n",
            "\n",
            "*************************************************\n",
            " Classificador 9 : Regressão Logística\n",
            " Acurácia: 0.9973333333333333\n",
            "\n",
            "*************************************************\n",
            " Classificador 10 : Regressão Linear\n",
            " Acurácia: 0.719899454340501\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUPV_14_1RxF"
      },
      "source": [
        "# ****************************************\n",
        "# A função rank_class rankeia os classificadores conforme suas acurácias\n",
        "from pprint import pprint\n",
        "def rank_class(xtotal, xnomes, xscor):\n",
        "    # xtotal : (inteiro) informa a quantidade de classificadores que serão rankeados\n",
        "    # xnomes : (lista) nomes dos classificadores\n",
        "    # xscor : (lista) valores dos scores de cada classificador\n",
        "    xdic_class_0 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_1 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_2 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_3 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_4 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_5 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_6 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_7 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_8 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xdic_class_9 = {'xclass' : 'classe', 'yscor' : -100.00}\n",
        "    xconta = 0\n",
        "    for q in range(xtotal):\n",
        "        if q == 0:\n",
        "          xdic_class_0 = {'xclass' : xnomes[0], 'yscor' : xscor[0]}\n",
        "        if q == 1:\n",
        "          xdic_class_1 = {'xclass' : xnomes[1], 'yscor' : xscor[1]}\n",
        "        if q == 2:\n",
        "          xdic_class_2 = {'xclass' : xnomes[2], 'yscor' : xscor[2]}\n",
        "        if q == 3:\n",
        "          xdic_class_3 = {'xclass' : xnomes[3], 'yscor' : xscor[3]}\n",
        "        if q == 4:\n",
        "          xdic_class_4 = {'xclass' : xnomes[4], 'yscor' : xscor[4]}\n",
        "        if q == 5:\n",
        "          xdic_class_5 = {'xclass' : xnomes[5], 'yscor' : xscor[5]}\n",
        "        if q == 6:\n",
        "          xdic_class_6 = {'xclass' : xnomes[6], 'yscor' : xscor[6]}\n",
        "        if q == 7:\n",
        "          xdic_class_7 = {'xclass' : xnomes[7], 'yscor' : xscor[7]}\n",
        "        if q == 8:\n",
        "          xdic_class_8 = {'xclass' : xnomes[8], 'yscor' : xscor[8]}\n",
        "        if q == 9:\n",
        "          xdic_class_9 = {'xclass' : xnomes[9], 'yscor' : xscor[9]}\n",
        "    xdic_class = [xdic_class_0, \n",
        "                  xdic_class_1, \n",
        "                  xdic_class_2, \n",
        "                  xdic_class_3,\n",
        "                  xdic_class_4,\n",
        "                  xdic_class_5,\n",
        "                  xdic_class_6,\n",
        "                  xdic_class_7,\n",
        "                  xdic_class_8,\n",
        "                  xdic_class_9]\n",
        "    print('Sem ordem')\n",
        "    pprint(xdic_class)\n",
        "    print('Com ordem')\n",
        "    xdic_ordenado = sorted(xdic_class, key=lambda k: k['yscor']) \n",
        "    pprint(xdic_ordenado)\n",
        "    xscor_class = []\n",
        "    xnome_class = []\n",
        "    xxvezes = 9\n",
        "    for i in range(10):\n",
        "      xdic=xdic_ordenado[xxvezes]\n",
        "      xscor_class.append(xdic['yscor'])\n",
        "      xnome_class.append(xdic['xclass'])\n",
        "      xxvezes -= 1\n",
        "    p_scor = xscor_class[0]\n",
        "    s_scor = t_scor = 0\n",
        "    n_prim = 0\n",
        "    n_seg = 0\n",
        "    n_ter = 0\n",
        "    ppp = 0\n",
        "    qqq = 0\n",
        "    zzz = 0\n",
        "    xscor_final = []\n",
        "    xnome_final =[]\n",
        "    for a in range(10):\n",
        "      if xscor_class[a] != -100.00:\n",
        "        if xscor_class[a] >= p_scor:\n",
        "            xscor_final.append(p_scor)\n",
        "            xnome_final.append(xnome_class[a])\n",
        "            n_prim += 1          \n",
        "        else:\n",
        "            ppp += 1\n",
        "            if ppp == 1:\n",
        "              s_scor = xscor_class[a]\n",
        "            if xscor_class[a] >= s_scor:\n",
        "              xscor_final.append(s_scor)\n",
        "              xnome_final.append(xnome_class[a])\n",
        "              n_seg += 1\n",
        "            else:\n",
        "              qqq += 1\n",
        "              if qqq == 1:\n",
        "                t_scor = xscor_class[a]\n",
        "              if xscor_class[a] >= t_scor:\n",
        "                xscor_final.append(t_scor)\n",
        "                xnome_final.append(xnome_class[a])\n",
        "                n_ter += 1\n",
        "    return xscor_final,xnome_final,n_prim,n_seg,n_ter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJbwpVnV-Akc",
        "outputId": "b1d85b0b-f675-47b3-9380-50c7a68c435e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xnomes = ['KNN', 'SVC', 'Processo Gaussiano', 'Árvores de Decisão', 'Floresta Aleatória', \n",
        "          'MLP', 'AdaBoost', 'Naive Bayes', 'Regressão Logística', 'Regressão Linear']\n",
        "xval, xpp, n_prim, n_seg, n_ter = rank_class(10, xnomes, xscores)\n",
        "xroda = n_prim+n_seg+n_ter\n",
        "xyn_prim = n_prim\n",
        "xyn_seg = n_seg\n",
        "xyn_ter = n_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores utilizando os vetores de recursos\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xyn_prim != 0:\n",
        "    if xyn_prim == n_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(xval[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+xpp[q])\n",
        "      xyn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+xpp[q])\n",
        "      xyn_prim -= 1\n",
        "  else:\n",
        "    if xyn_seg != 0:\n",
        "      if xyn_seg == n_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(xval[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+xpp[q])\n",
        "        xyn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+xpp[q])\n",
        "        xyn_seg -= 1\n",
        "    else:\n",
        "      if xyn_ter != 0:\n",
        "        if xyn_ter == n_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(xval[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+xpp[q])\n",
        "          xyn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+xpp[q])\n",
        "          xyn_ter -= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sem ordem\n",
            "[{'xclass': 'KNN', 'yscor': 0.9893333333333333},\n",
            " {'xclass': 'SVC', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Processo Gaussiano', 'yscor': 0.042666666666666665},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.28},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.8026666666666666},\n",
            " {'xclass': 'MLP', 'yscor': 0.9946666666666667},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.18666666666666668},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.9173333333333333},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501}]\n",
            "Com ordem\n",
            "[{'xclass': 'Processo Gaussiano', 'yscor': 0.042666666666666665},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.18666666666666668},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.28},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.8026666666666666},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.9173333333333333},\n",
            " {'xclass': 'KNN', 'yscor': 0.9893333333333333},\n",
            " {'xclass': 'MLP', 'yscor': 0.9946666666666667},\n",
            " {'xclass': 'SVC', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.9973333333333333}]\n",
            "------------------------------------------------------------------------\n",
            "************************************************************************\n",
            "Rankeamento dos Classificadores utilizando os vetores de recursos\n",
            "************************************************************************\n",
            "Maior Acurácia: 0.9973333333333333\n",
            "Classificador(es) com maior acurácia:\n",
            "1. Regressão Logística\n",
            "2. SVC\n",
            "************************************************************************\n",
            "Segunda Maior Acurácia: 0.9946666666666667\n",
            "Classificador(es) com a segunda maior acurácia:\n",
            "1. MLP\n",
            "************************************************************************\n",
            "Terceira Maior Acurácia: 0.9893333333333333\n",
            "Classificador(es) com a terceira maior acurácia:\n",
            "1. KNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eK1T3vV3K2b",
        "outputId": "e748f214-af2c-4029-9838-0dd2af5e220f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Ajustando os classificadores usando GridSearchCV\n",
        "##################################################\n",
        "\n",
        "# Regressão Logística\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo LogR\n",
        "LogR = LogisticRegression()\n",
        "# Criando um dicionário com as variações do parâmetro normalize\n",
        "params_LogR = {'random_state': [1, 2, 3, 4, 5],\n",
        "               'max_iter': [1000, 1500, 2000, 2500, 3000]}\n",
        "\n",
        "#  Quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "LogR_gs = GridSearchCV(LogR, params_LogR, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "LogR_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "LogR_best = LogR_gs.best_estimator_\n",
        "\n",
        "# LogR_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "LogR_scor = LogR_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_LogR e xxv_LogR serão utilizados como recursos para o empilhamento total\n",
        "xxtreino_LogR = LogR_best.predict(xxt)\n",
        "xxt_LogR = pd.DataFrame(xxtreino_LogR)\n",
        "xxvalid_LogR = LogR_best.predict(xxvd)\n",
        "xxv_LogR = pd.DataFrame(xxvalid_LogR)\n",
        "\n",
        "\n",
        "# K-NN (K vizinhos mais próximos)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo KNN\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Criando um dicionário com as variações do parâmetro n_neighbors (número de vizinhos) a ser testado\n",
        "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
        "\n",
        "# Usando GridSearchCV para testar todos os valores de vizinhos (neighbors),\n",
        "#  quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "knn_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "knn_best = knn_gs.best_estimator_\n",
        "\n",
        "# knn_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "knn_scor = knn_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_knn e xxv_knn serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_knn = knn_best.predict(xxt)\n",
        "xxt_knn = pd.DataFrame(xxtreino_knn)\n",
        "xxvalid_knn = knn_best.predict(xxvd)\n",
        "xxv_knn = pd.DataFrame(xxvalid_knn)\n",
        "\n",
        "# Floresta Aleatória (Random Forest)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do tipo Floresta Aleatória\n",
        "rf = RandomForestClassifier(max_depth=5, max_features=1)\n",
        "\n",
        "# O hiperparâmetro n_estimators indica o número de árvores construídas pelo algoritmo antes de realizar\n",
        "#   uma votação ou fazer uma média de predições.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para n_estimators\n",
        "params_rf = {'n_estimators': [25, 50, 75, 100, 125, 150]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "rf_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "rf_best = rf_gs.best_estimator_\n",
        "\n",
        "# rf_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "rf_scor = rf_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_rf e xxv_rf serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_rf = rf_best.predict(xxt)\n",
        "xxt_rf = pd.DataFrame(xxtreino_rf)\n",
        "xxvalid_rf = rf_best.predict(xxvd)\n",
        "xxv_rf = pd.DataFrame(xxvalid_rf)\n",
        "\n",
        "# MLP: MultiLayer Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do MLP\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Alfa é um parâmetro para o termo de regularização, também conhecido como termo de penalidade,\n",
        "#    que combate o overfitting ao restringir o tamanho dos pesos.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para alpha\n",
        "params_mlp = {'alpha': [0.1, 0.2, 0.3, 0.4],\n",
        "               'max_iter': [1000, 1500, 2000, 2500]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "mlp_gs = GridSearchCV(mlp, params_mlp, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "mlp_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "mlp_best = mlp_gs.best_estimator_\n",
        "\n",
        "# mlp_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "mlp_scor = mlp_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_mlp e xxv_mlp serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_mlp = mlp_best.predict(xxt)\n",
        "xxt_mlp = pd.DataFrame(xxtreino_mlp)\n",
        "xxvalid_mlp = mlp_best.predict(xxvd)\n",
        "xxv_mlp = pd.DataFrame(xxvalid_mlp)\n",
        "\n",
        "# AB: AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do MLP\n",
        "cl_ada = AdaBoostClassifier()\n",
        "\n",
        "# n_estimators int, padrão = 50\n",
        "# O número máximo de estimadores nos quais o aumento é finalizado. \n",
        "# Em caso de ajuste perfeito, o procedimento de aprendizado é interrompido mais cedo.\n",
        "params_ada = {'n_estimators': [30, 35, 40, 45, 50, 55, 65, 70]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "ada_gs = GridSearchCV(cl_ada, params_ada, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "ada_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "ada_best = ada_gs.best_estimator_\n",
        "\n",
        "# ada_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "ada_scor = ada_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_ada e xxv_ada serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_ada = ada_best.predict(xxt)\n",
        "xxt_ada = pd.DataFrame(xxtreino_ada)\n",
        "xxvalid_ada = ada_best.predict(xxvd)\n",
        "xxv_ada = pd.DataFrame(xxvalid_ada)\n",
        "\n",
        "\n",
        "# GNB: GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Praticamente não há parâmetros para variar\n",
        "params_gnb = {'priors': [None]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "gnb_gs = GridSearchCV(gnb, params_gnb, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "gnb_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "gnb_best = gnb_gs.best_estimator_\n",
        "\n",
        "# gnb_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "gnb_scor = gnb_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_ada e xxv_ada serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_gnb = gnb_best.predict(xxt)\n",
        "xxt_gnb = pd.DataFrame(xxtreino_gnb)\n",
        "xxvalid_gnb = gnb_best.predict(xxvd)\n",
        "xxv_gnb = pd.DataFrame(xxvalid_gnb)\n",
        "\n",
        "\n",
        "# Classificador com vetores de suporte (SVC)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "\n",
        "cl_svc = SVC()\n",
        "\n",
        "# Vamos variar dois parâmetros dessa vez, o kernel e o parâmetro de regularização C\n",
        "\n",
        "params_svc = {'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1], 'kernel':['linear', 'sigmoid']}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "\n",
        "svc_gs = GridSearchCV(cl_svc, params_svc, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "\n",
        "svc_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "\n",
        "svc_best = svc_gs.best_estimator_\n",
        "\n",
        "# svc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "svc_scor = svc_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_ada e xxv_ada serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_svc = svc_best.predict(xxt)\n",
        "xxt_svc = pd.DataFrame(xxtreino_svc)\n",
        "xxvalid_svc = svc_best.predict(xxvd)\n",
        "xxv_svc = pd.DataFrame(xxvalid_svc)\n",
        "\n",
        "\n",
        "# Classificação de processo Gaussiana (GPC) com base na aproximação de Laplace.\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import pandas as pd\n",
        "\n",
        "# RBF: Kernel de função de base radial (também conhecido como kernel quadrado-exponencial).\n",
        "# Seu parâmetro length_scale é padronizado para 1.0\n",
        "kernel1 = 0.90 * RBF (1.0)\n",
        "kernel2 = 0.95 * RBF (1.0)\n",
        "kernel3 = 1.0 * RBF (1.0)\n",
        "\n",
        "# Criando um classificador GPC\n",
        "cl_gpc = GaussianProcessClassifier()\n",
        "\n",
        "# Vamos variar o kernel desse classificador, veja a biblioteca\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html\n",
        "# para maiores detalhes desse parâmetro\n",
        "params_gpc = {'kernel':[kernel1, kernel2, kernel3]}\n",
        "                        \n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "gpc_gs = GridSearchCV(cl_gpc, params_gpc, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "gpc_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "gpc_best = gpc_gs.best_estimator_\n",
        "\n",
        "# gpc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "gpc_scor = gpc_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_gpc e xxv_gpc serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_gpc = gpc_best.predict(xxt)\n",
        "xxt_gpc = pd.DataFrame(xxtreino_gpc)\n",
        "xxvalid_gpc = gpc_best.predict(xxvd)\n",
        "xxv_gpc = pd.DataFrame(xxvalid_gpc)\n",
        "\n",
        "# Classificador com arvores de decisão (DTC)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "cl_dtc = DecisionTreeClassifier()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "params_dtc = {'max_depth':[3, 5, 7, 9, 11, 13, 15]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "dtc_gs = GridSearchCV(cl_dtc, params_dtc, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "dtc_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "dtc_best = dtc_gs.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "dtc_scor = dtc_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_dtc e xxv_dtc serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_dtc = dtc_best.predict(xxt)\n",
        "xxt_dtc = pd.DataFrame(xxtreino_dtc)\n",
        "xxvalid_dtc = dtc_best.predict(xxvd)\n",
        "xxv_dtc = pd.DataFrame(xxvalid_dtc)\n",
        "\n",
        "\n",
        "# Classificador por regressão linear\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "cl_LR = LinearRegression()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "params_LR = {'normalize':[True, False]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "LR_gs = GridSearchCV(cl_LR, params_LR, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "LR_gs.fit(xxt, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "LR_best = LR_gs.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "LR_scor = LR_best.score(xxvd, xxlv)\n",
        "\n",
        "# xxt_LR e xxv_LR serão utilizados \n",
        "# como recursos para o empilhamento total\n",
        "xxtreino_LR = LR_best.predict(xxt)\n",
        "xxt_LR = pd.DataFrame(xxtreino_LR)\n",
        "xxvalid_LR = LR_best.predict(xxvd)\n",
        "xxv_LR = pd.DataFrame(xxvalid_LR)\n",
        "\n",
        "# Scores dos classificadores com parâmetros ajustados\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia das árvores de decisão com parâmetro ajustado: \" +str(dtc_scor)) #1\n",
        "print(\"Parâmetro ajustado: \"+str(dtc_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do Processo Gaussiano com parâmetro ajustado: \" +str(gpc_scor)) #2\n",
        "print(\"Parâmetro ajustado: \"+str(gpc_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do SVC com parâmetros ajustados: \" +str(svc_scor)) #3\n",
        "print(\"Parâmetros ajustados: \"+str(svc_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do Naive Bayes: \" +str(gnb_scor))\n",
        "print(\" Para esse classificador não ajustamos parâmetros.\") #4\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do AdaBoost com parâmetro ajustado: \" +str(ada_scor)) #5\n",
        "print(\"Parâmetro ajustado: \"+str(ada_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do MLP com parâmetros ajustados: \" +str(mlp_scor)) #6\n",
        "print(\"Parâmetros ajustados: \"+str(mlp_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Floresta Aleatória com parâmetro ajustado: \" +str(rf_scor)) #7\n",
        "print(\"Parâmetro ajustado: \"+str(rf_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do KNN com parâmetro ajustado: \" +str(knn_scor)) #8\n",
        "print(\"Parâmetro ajustado: \"+str(knn_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Regressão Logísica com parâmetros ajustados: \" +str(LogR_scor)) #9\n",
        "print(\"Parâmetros ajustados: \"+str(LogR_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da regressão linear com parâmetro ajustado: \" +str(LR_scor)) #10\n",
        "print(\"Parâmetro ajustado: \"+str(LR_gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*********************************************************************************\n",
            "Acurácia das árvores de decisão com parâmetro ajustado: 0.6826666666666666\n",
            "Parâmetro ajustado: {'max_depth': 15}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do Processo Gaussiano com parâmetro ajustado: 0.042666666666666665\n",
            "Parâmetro ajustado: {'kernel': 0.949**2 * RBF(length_scale=1)}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do SVC com parâmetros ajustados: 0.9973333333333333\n",
            "Parâmetros ajustados: {'C': 0.1, 'kernel': 'linear'}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do Naive Bayes: 0.9173333333333333\n",
            " Para esse classificador não ajustamos parâmetros.\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do AdaBoost com parâmetro ajustado: 0.144\n",
            "Parâmetro ajustado: {'n_estimators': 65}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do MLP com parâmetros ajustados: 0.9973333333333333\n",
            "Parâmetros ajustados: {'alpha': 0.2, 'max_iter': 1000}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia da Floresta Aleatória com parâmetro ajustado: 0.9893333333333333\n",
            "Parâmetro ajustado: {'n_estimators': 150}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do KNN com parâmetro ajustado: 0.992\n",
            "Parâmetro ajustado: {'n_neighbors': 1}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia da Regressão Logísica com parâmetros ajustados: 0.9973333333333333\n",
            "Parâmetros ajustados: {'max_iter': 1000, 'random_state': 1}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia da regressão linear com parâmetro ajustado: 0.719899454340501\n",
            "Parâmetro ajustado: {'normalize': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54jzc5sPwzLE",
        "outputId": "a6d17240-ebba-41b8-db98-f071443bfb1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xnomes = ['KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', 'Naive Bayes', \n",
        "          'SVC', 'Processo Gaussiano', 'Árvores de Decisão', 'Regressão Logística', 'Regressão Linear']\n",
        "xscor = [knn_scor, rf_scor, mlp_scor, ada_scor, gnb_scor, svc_scor, gpc_scor, dtc_scor, LogR_scor, LR_scor]\n",
        "xscor1, xprim, xn_prim, xn_seg, xn_ter = rank_class(10, xnomes, xscor)\n",
        "xroda = n_prim+n_seg+n_ter\n",
        "xxn_prim = xn_prim\n",
        "xxn_seg = xn_seg\n",
        "xxn_ter = xn_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores, utilizando Vetores de Recurso e com parâmetros ajustados\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xxn_prim != 0:\n",
        "    if xxn_prim == xn_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(xscor1[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+xprim[q])\n",
        "      xxn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+xprim[q])\n",
        "      xxn_prim -= 1\n",
        "  else:\n",
        "    if xxn_seg != 0:\n",
        "      if xxn_seg == xn_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(xscor1[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+xprim[q])\n",
        "        xxn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+xprim[q])\n",
        "        xxn_seg -= 1\n",
        "    else:\n",
        "      if xxn_ter != 0:\n",
        "        if xxn_ter == xn_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(xscor1[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+xprim[q])\n",
        "          xxn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+xprim[q])\n",
        "          xxn_ter -= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sem ordem\n",
            "[{'xclass': 'KNN', 'yscor': 0.992},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.9893333333333333},\n",
            " {'xclass': 'MLP', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.144},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.9173333333333333},\n",
            " {'xclass': 'SVC', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Processo Gaussiano', 'yscor': 0.042666666666666665},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.6826666666666666},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501}]\n",
            "Com ordem\n",
            "[{'xclass': 'Processo Gaussiano', 'yscor': 0.042666666666666665},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.144},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.6826666666666666},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.9173333333333333},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.9893333333333333},\n",
            " {'xclass': 'KNN', 'yscor': 0.992},\n",
            " {'xclass': 'MLP', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'SVC', 'yscor': 0.9973333333333333},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.9973333333333333}]\n",
            "------------------------------------------------------------------------\n",
            "************************************************************************\n",
            "Rankeamento dos Classificadores, utilizando Vetores de Recurso e com parâmetros ajustados\n",
            "************************************************************************\n",
            "Maior Acurácia: 0.9973333333333333\n",
            "Classificador(es) com maior acurácia:\n",
            "1. Regressão Logística\n",
            "2. SVC\n",
            "3. MLP\n",
            "************************************************************************\n",
            "Segunda Maior Acurácia: 0.992\n",
            "Classificador(es) com a segunda maior acurácia:\n",
            "1. KNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ4Av1K73WPS",
        "outputId": "356e14db-5e32-444d-bd58-68efcaee9077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Realizando a votação das classificações\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Criando um dicionário com nossos modelos\n",
        "# observação: VotingClassifier não aceita regressão lienar como classificador\n",
        "\n",
        "estimators = [('knn', knn_best), \n",
        "              ('rf', rf_best), \n",
        "              ('mlp', mlp_best), \n",
        "              ('ada', ada_best), \n",
        "              ('gnb', gnb_best), \n",
        "              ('svc', svc_best), \n",
        "              ('gpc', gpc_best), \n",
        "              ('dtc', dtc_best),\n",
        "              ('LogR', LogR_best)]\n",
        "\n",
        "# Criando a classificação por votação\n",
        "\n",
        "ensemble = VotingClassifier(estimators, voting='hard')\n",
        "ensemble.fit(xxt, xxlt)\n",
        "xensemb = ensemble.score(xxvd, xxlv)\n",
        "\n",
        "# Exibindo a acurácia que obteve maior número de votos\n",
        "print(\"Acurácia da votação entre classificadores: \"+str(xensemb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia da votação entre classificadores: 0.9973333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAuqscOe4Kj5"
      },
      "source": [
        "# Empilhamento TOTAL\n",
        "import pandas as pd\n",
        "xx_treino = pd.concat([xxt_knn, \n",
        "                       xxt_rf,\n",
        "                       xxt_mlp, \n",
        "                       xxt_ada, \n",
        "                       xxt_gnb, \n",
        "                       xxt_svc, \n",
        "                       xxt_gpc, \n",
        "                       xxt_dtc,\n",
        "                       xxt_LogR,\n",
        "                       xxt_LR], axis = 1)\n",
        "\n",
        "xx_valid = pd.concat([xxv_knn, \n",
        "                      xxv_rf, \n",
        "                      xxv_mlp, \n",
        "                      xxv_ada, \n",
        "                      xxv_gnb, \n",
        "                      xxv_svc, \n",
        "                      xxv_gpc, \n",
        "                      xxv_dtc,\n",
        "                      xxv_LogR,\n",
        "                      xxv_LR], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CzZZx1G4RRJ",
        "outputId": "15bfb625-e887-4ef9-94e8-a239de1851e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Classificação com empilhamento total \n",
        "# Usando GridSearchCV para ajustar os classificadores\n",
        "######################################################\n",
        "\n",
        "# Regressão Logística\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo LogR\n",
        "LogR2 = LogisticRegression()\n",
        "# Criando um dicionário com as variações do parâmetro normalize\n",
        "params_LogR2 = {'random_state': [1, 2, 3, 4, 5],\n",
        "               'max_iter': [1000, 1500, 2000, 2500, 3000]}\n",
        "\n",
        "#  Quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "LogR_gs2 = GridSearchCV(LogR2, params_LogR2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "LogR_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "LogR_best2 = LogR_gs2.best_estimator_\n",
        "\n",
        "# LogR_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "LogR_scor2 = LogR_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# K-NN (K vizinhos mais próximos)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo KNN\n",
        "knn2 = KNeighborsClassifier()\n",
        "\n",
        "# Criando um dicionário com as variações do parâmetro n_neighbors (número de vizinhos) a ser testado\n",
        "params_knn2 = {'n_neighbors': np.arange(1, 25)}\n",
        "\n",
        "# Usando GridSearchCV para testar todos os valores de vizinhos (neighbors),\n",
        "#  quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "knn_gs2 = GridSearchCV(knn2, params_knn2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "knn_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "knn_best2 = knn_gs2.best_estimator_\n",
        "\n",
        "# knn_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "knn_scor2 = knn_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# Floresta Aleatória (Random Forest)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do tipo Floresta Aleatória\n",
        "rf2 = RandomForestClassifier(max_depth=5, max_features=1)\n",
        "\n",
        "# O hiperparâmetro n_estimators indica o número de árvores construídas pelo algoritmo antes de realizar\n",
        "#   uma votação ou fazer uma média de predições.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para n_estimators\n",
        "params_rf2 = {'n_estimators': [25, 50, 75, 100, 125, 150]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "rf_gs2 = GridSearchCV(rf2, params_rf2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "rf_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "rf_best2 = rf_gs2.best_estimator_\n",
        "\n",
        "# rf_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "rf_scor2 = rf_best2.score(xx_valid, xxlv)\n",
        "\n",
        "\n",
        "# MLP: MultiLayer Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do MLP\n",
        "mlp2 = MLPClassifier()\n",
        "\n",
        "# Alfa é um parâmetro para o termo de regularização, também conhecido como termo de penalidade,\n",
        "#    que combate o overfitting ao restringir o tamanho dos pesos.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para alpha\n",
        "params_mlp2 = {'alpha': [0.1, 0.2, 0.3, 0.4],\n",
        "               'max_iter': [1000, 1500, 2000, 2500]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "mlp_gs2 = GridSearchCV(mlp2, params_mlp2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "mlp_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "mlp_best2 = mlp_gs2.best_estimator_\n",
        "\n",
        "# mlp_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "mlp_scor2 = mlp_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# AB: AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do ADA\n",
        "cl_ada2 = AdaBoostClassifier()\n",
        "\n",
        "# n_estimators int, padrão = 50\n",
        "# O número máximo de estimadores nos quais o aumento é finalizado. \n",
        "# Em caso de ajuste perfeito, o procedimento de aprendizado é interrompido mais cedo.\n",
        "params_ada2 = {'n_estimators': [30, 35, 40, 45, 50, 55, 65, 70]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "ada_gs2 = GridSearchCV(cl_ada2, params_ada2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "ada_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "ada_best2 = ada_gs2.best_estimator_\n",
        "\n",
        "# ada_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "ada_scor2 = ada_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# GNB: GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador\n",
        "gnb2 = GaussianNB()\n",
        "\n",
        "# Praticamente não há parâmetros para variar\n",
        "params_gnb2 = {'priors': [None]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "gnb_gs2 = GridSearchCV(gnb2, params_gnb2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "gnb_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "gnb_best2 = gnb_gs2.best_estimator_\n",
        "\n",
        "# ada_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "gnb_scor2 = gnb_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# Classificador com vetores de suporte (SVC)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "\n",
        "cl_svc2 = SVC()\n",
        "\n",
        "# Vamos variar dois parâmetros dessa vez, o kernel e o parâmetro de regularização C\n",
        "\n",
        "params_svc2 = {'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1], 'kernel':['linear', 'sigmoid']}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "\n",
        "svc_gs2 = GridSearchCV(cl_svc2, params_svc2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "\n",
        "svc_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "\n",
        "svc_best2 = svc_gs2.best_estimator_\n",
        "\n",
        "# svc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "svc_scor2 = svc_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# Classificador com arvores de decisão (DTC)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "cl_dtc2 = DecisionTreeClassifier()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "params_dtc2 = {'max_depth':[3, 5, 7, 9, 11, 13, 15]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "dtc_gs2 = GridSearchCV(cl_dtc2, params_dtc2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "dtc_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "dtc_best2 = dtc_gs2.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "dtc_scor2 = dtc_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# Classificador por regressão linear\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "cl_LR2 = LinearRegression()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "params_LR2 = {'normalize':[True, False]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "LR_gs2 = GridSearchCV(cl_LR2, params_LR2, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "LR_gs2.fit(xx_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "LR_best2 = LR_gs2.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "LR_scor2 = LR_best2.score(xx_valid, xxlv)\n",
        "\n",
        "# Scores com os classificadores ajustados\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia das árvores de decisão com parâmetro ajustado: \" +str(dtc_scor2))\n",
        "print(\"Parâmetro ajustado: \"+str(dtc_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do SVC com parâmetros ajustados: \" +str(svc_scor2))\n",
        "print(\"Parâmetros ajustados: \"+str(svc_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do Naive Bayes: \" +str(gnb_scor2))\n",
        "print(\" Para esse classificador não ajustamos parâmetros.\")\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do AdaBoost com parâmetro ajustado: \" +str(ada_scor2))\n",
        "print(\"Parâmetros ajustado: \"+str(ada_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do MLP com parâmetro ajustado: \" +str(mlp_scor2))\n",
        "print(\"Parâmetro ajustado: \"+str(mlp_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Floresta Aleatória com parâmetro ajustado: \" +str(rf_scor2))\n",
        "print(\"Parâmetro ajustado: \"+str(rf_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do KNN com parâmetro ajustado: \" +str(knn_scor2))\n",
        "print(\"Parâmetro ajustado: \"+str(knn_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Regressão Logísica com parâmetros ajustados: \" +str(LogR_scor2))\n",
        "print(\"Parâmetros ajustados: \"+str(LogR_gs2.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia das árvores de decisão com parâmetro ajustado: \" +str(LR_scor2))\n",
        "print(\"Parâmetro ajustado: \"+str(LR_gs2.best_params_))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*********************************************************************************\n",
            "Acurácia das árvores de decisão com parâmetro ajustado: 0.33866666666666667\n",
            "Parâmetro ajustado: {'max_depth': 11}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do SVC com parâmetros ajustados: 0.19733333333333333\n",
            "Parâmetros ajustados: {'C': 0.2, 'kernel': 'linear'}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do Naive Bayes: 0.232\n",
            " Para esse classificador não ajustamos parâmetros.\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do AdaBoost com parâmetro ajustado: 0.33866666666666667\n",
            "Parâmetros ajustado: {'n_estimators': 70}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do MLP com parâmetro ajustado: 0.24533333333333332\n",
            "Parâmetro ajustado: {'alpha': 0.2, 'max_iter': 2000}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia da Floresta Aleatória com parâmetro ajustado: 0.928\n",
            "Parâmetro ajustado: {'n_estimators': 100}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia do KNN com parâmetro ajustado: 0.22933333333333333\n",
            "Parâmetro ajustado: {'n_neighbors': 1}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia da Regressão Logísica com parâmetros ajustados: 0.304\n",
            "Parâmetros ajustados: {'max_iter': 3000, 'random_state': 1}\n",
            "\n",
            "*********************************************************************************\n",
            "Acurácia das árvores de decisão com parâmetro ajustado: 0.9159788115075632\n",
            "Parâmetro ajustado: {'normalize': True}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJecEjrc0v7e",
        "outputId": "bad534fa-a1ff-481d-c9f1-ecdb04682eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xnomes = ['KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', 'Naive Bayes', 'SVC',\n",
        "          'Árvores de Decisão', 'Regressão Logística', 'Regressão Linear']\n",
        "xscor = [knn_scor2, rf_scor2, mlp_scor2, ada_scor2, gnb_scor2, svc_scor2, dtc_scor2, LogR_scor2, LR_scor]\n",
        "et_scor, et_prim, et_n_prim, et_n_seg, et_n_ter = rank_class(9, xnomes, xscor)\n",
        "xroda = et_n_prim+et_n_seg+et_n_ter\n",
        "xxn_prim = et_n_prim\n",
        "xxn_seg = et_n_seg\n",
        "xxn_ter = et_n_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores com empilhamento total\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xxn_prim != 0:\n",
        "    if xxn_prim == et_n_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(et_scor[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+et_prim[q])\n",
        "      xxn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+et_prim[q])\n",
        "      xxn_prim -= 1\n",
        "  else:\n",
        "    if xxn_seg != 0:\n",
        "      if xxn_seg == et_n_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(et_scor[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+et_prim[q])\n",
        "        xxn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+et_prim[q])\n",
        "        xxn_seg -= 1\n",
        "    else:\n",
        "      if xxn_ter != 0:\n",
        "        if xxn_ter == et_n_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(et_scor[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+et_prim[q])\n",
        "          xxn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+et_prim[q])\n",
        "          xxn_ter -= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sem ordem\n",
            "[{'xclass': 'KNN', 'yscor': 0.22933333333333333},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.928},\n",
            " {'xclass': 'MLP', 'yscor': 0.24533333333333332},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.33866666666666667},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.232},\n",
            " {'xclass': 'SVC', 'yscor': 0.19733333333333333},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.33866666666666667},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.304},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501},\n",
            " {'xclass': 'classe', 'yscor': -100.0}]\n",
            "Com ordem\n",
            "[{'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'SVC', 'yscor': 0.19733333333333333},\n",
            " {'xclass': 'KNN', 'yscor': 0.22933333333333333},\n",
            " {'xclass': 'Naive Bayes', 'yscor': 0.232},\n",
            " {'xclass': 'MLP', 'yscor': 0.24533333333333332},\n",
            " {'xclass': 'Regressão Logística', 'yscor': 0.304},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.33866666666666667},\n",
            " {'xclass': 'Árvores de Decisão', 'yscor': 0.33866666666666667},\n",
            " {'xclass': 'Regressão Linear', 'yscor': 0.719899454340501},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.928}]\n",
            "------------------------------------------------------------------------\n",
            "************************************************************************\n",
            "Rankeamento dos Classificadores com empilhamento total\n",
            "************************************************************************\n",
            "Maior Acurácia: 0.928\n",
            "Classificador(es) com maior acurácia:\n",
            "1. Floresta Aleatória\n",
            "************************************************************************\n",
            "Segunda Maior Acurácia: 0.719899454340501\n",
            "Classificador(es) com a segunda maior acurácia:\n",
            "1. Regressão Linear\n",
            "************************************************************************\n",
            "Terceira Maior Acurácia: 0.33866666666666667\n",
            "Classificador(es) com a terceira maior acurácia:\n",
            "1. Árvores de Decisão\n",
            "2. AdaBoost\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne4MOD8D403F",
        "outputId": "238d09b1-8f4f-40a4-ce2c-59df27ba2c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Classificação com empilhamento seletivo\n",
        "import pandas as pd\n",
        "xclassif = ['KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', 'Naive Bayes', 'SVC', \n",
        "            'Árvores de Decisão', 'Regressão Logística', 'Regressão Linear']\n",
        "clf2 = [knn_best2,\n",
        "        rf_best2,\n",
        "        mlp_best2,\n",
        "        ada_best2,\n",
        "        svc_best2,\n",
        "        gnb_best2,\n",
        "        dtc_best2,\n",
        "        LogR_best2,\n",
        "        LR_best2]\n",
        "xt_classif =[xxt_knn,\n",
        "             xxt_rf,\n",
        "             xxt_mlp,\n",
        "             xxt_ada,\n",
        "             xxt_svc,\n",
        "             xxt_gnb,\n",
        "             xxt_dtc,\n",
        "             xxt_LogR,\n",
        "             xxt_LR]\n",
        "xv_classif =[xxv_knn,\n",
        "             xxv_rf,\n",
        "             xxv_mlp,\n",
        "             xxv_ada,\n",
        "             xxv_svc,\n",
        "             xxv_gnb,\n",
        "             xxv_dtc,\n",
        "             xxv_LogR,\n",
        "             xxv_LR]\n",
        "classif = []\n",
        "x=0\n",
        "clf3 = []\n",
        "yhj = 0\n",
        "n_class_et = et_n_prim+et_n_seg+et_n_ter\n",
        "for x in range(n_class_et):\n",
        "  for y in range(9):\n",
        "    if et_prim[x] == xclassif[y]:\n",
        "        classif.append(xclassif[y])\n",
        "        clf3.append(clf2[y])\n",
        "        yhj += 1\n",
        "ptreino = []\n",
        "pval = []\n",
        "xv1 = 0\n",
        "xn_class = xn_prim+xn_seg+xn_ter\n",
        "for x in range(xn_class):\n",
        "  for y in range(9):\n",
        "    if xprim[x] == xclassif[y]:\n",
        "        xv1 += 1\n",
        "        ptreino.append(xt_classif[y])\n",
        "        pval.append(xv_classif[y])\n",
        "es_treino = []\n",
        "es_valid = []\n",
        "for k in range(xv1):\n",
        "  es_treino.append(ptreino[k])\n",
        "  es_valid.append(pval[k])\n",
        "xx_treino = pd.concat(es_treino, axis = 1)\n",
        "xx_valid = pd.concat(es_valid, axis = 1)\n",
        "i = 0\n",
        "xs_scores = []\n",
        "for name, clf in zip(classif, clf3):\n",
        "        clf.fit(xx_treino, xxlt)\n",
        "        score = clf.score(xx_valid, xxlv)\n",
        "        xs_scores.append(score)\n",
        "        i += 1\n",
        "es_scor, es_prim, es_n_prim, es_n_seg, es_n_ter = rank_class(i, xnomes, xs_scores)\n",
        "xroda = es_n_prim+es_n_seg+es_n_ter\n",
        "xxn_prim = es_n_prim\n",
        "xxn_seg = es_n_seg\n",
        "xxn_ter = es_n_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores com empilhamento seletivo\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xxn_prim != 0:\n",
        "    if xxn_prim == es_n_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(es_scor[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+es_prim[q])\n",
        "      xxn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+es_prim[q])\n",
        "      xxn_prim -= 1\n",
        "  else:\n",
        "    if xxn_seg != 0:\n",
        "      if xxn_seg == es_n_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(es_scor[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+es_prim[q])\n",
        "        xxn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+es_prim[q])\n",
        "        xxn_seg -= 1\n",
        "    else:\n",
        "      if xxn_ter != 0:\n",
        "        if xxn_ter == es_n_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(es_scor[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+es_prim[q])\n",
        "          xxn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+es_prim[q])\n",
        "          xxn_ter -= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sem ordem\n",
            "[{'xclass': 'KNN', 'yscor': 0.9626666666666667},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.9933458469038406},\n",
            " {'xclass': 'MLP', 'yscor': 0.9733333333333334},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.3893333333333333},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0}]\n",
            "Com ordem\n",
            "[{'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'classe', 'yscor': -100.0},\n",
            " {'xclass': 'AdaBoost', 'yscor': 0.3893333333333333},\n",
            " {'xclass': 'KNN', 'yscor': 0.9626666666666667},\n",
            " {'xclass': 'MLP', 'yscor': 0.9733333333333334},\n",
            " {'xclass': 'Floresta Aleatória', 'yscor': 0.9933458469038406}]\n",
            "------------------------------------------------------------------------\n",
            "************************************************************************\n",
            "Rankeamento dos Classificadores com empilhamento seletivo\n",
            "************************************************************************\n",
            "Maior Acurácia: 0.9933458469038406\n",
            "Classificador(es) com maior acurácia:\n",
            "1. Floresta Aleatória\n",
            "************************************************************************\n",
            "Segunda Maior Acurácia: 0.9733333333333334\n",
            "Classificador(es) com a segunda maior acurácia:\n",
            "1. MLP\n",
            "************************************************************************\n",
            "Terceira Maior Acurácia: 0.9626666666666667\n",
            "Classificador(es) com a terceira maior acurácia:\n",
            "1. KNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ndUyrE49CX"
      },
      "source": [
        "# Mistura total\n",
        "import pandas as pd\n",
        "m_xxt = pd.DataFrame(xxt)\n",
        "m_xxvd = pd.DataFrame(xxvd)\n",
        "xm_treino = pd.concat([xx_treino, \n",
        "                       m_xxt], axis = 1)\n",
        "\n",
        "xm_valid = pd.concat([xx_valid, \n",
        "                      m_xxvd], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA6hPpB45FFd",
        "outputId": "91249c20-e44a-4de6-a562-500b0f4a76b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Classificação com mistura total \n",
        "# Usando GridSearchCV para ajustar os classificadores\n",
        "######################################################\n",
        "\n",
        "# Regressão Logística\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo LogR\n",
        "m_LogR = LogisticRegression()\n",
        "# Criando um dicionário com as variações dos parâmetros a serem testados\n",
        "m_params_LogR = {'random_state': [1, 2, 3, 4, 5],\n",
        "               'max_iter': [1000, 1500, 2000, 2500, 3000]}\n",
        "\n",
        "#  Quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "m_LogR_gs = GridSearchCV(m_LogR, m_params_LogR, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_LogR_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_LogR_best = m_LogR_gs.best_estimator_\n",
        "\n",
        "# LogR_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_LogR_scor = m_LogR_best.score(xm_valid, xxlv)\n",
        "\n",
        "# K-NN (K vizinhos mais próximos)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo modelo KNN\n",
        "m_knn = KNeighborsClassifier()\n",
        "\n",
        "# Criando um dicionário com as variações do parâmetro n_neighbors (número de vizinhos) a ser testado\n",
        "m_params_knn = {'n_neighbors': np.arange(1, 25)}\n",
        "\n",
        "# Usando GridSearchCV para testar todos os valores de vizinhos (neighbors),\n",
        "#  quando cv é um número inteiro o GridSearchCV executa um StratifiedKFolds, \n",
        "#  isso quer dizer que o dataset foi divido em 5 partes (ou folds) \n",
        "#  e cada parte foi usada como test em uma simulação.\n",
        "m_knn_gs = GridSearchCV(m_knn, m_params_knn, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_knn_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_knn_best = m_knn_gs.best_estimator_\n",
        "\n",
        "# knn_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_knn_scor = m_knn_best.score(xm_valid, xxlv)\n",
        "\n",
        "# Floresta Aleatória (Random Forest)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do tipo Floresta Aleatória\n",
        "m_rf = RandomForestClassifier(max_depth=5, max_features=1)\n",
        "\n",
        "# O hiperparâmetro n_estimators indica o número de árvores construídas pelo algoritmo antes de realizar\n",
        "#   uma votação ou fazer uma média de predições.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para n_estimators\n",
        "m_params_rf = {'n_estimators': [25, 50, 75, 100, 125, 150]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "m_rf_gs = GridSearchCV(m_rf, m_params_rf, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_rf_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_rf_best = m_rf_gs.best_estimator_\n",
        "\n",
        "# rf_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_rf_scor = m_rf_best.score(xm_valid, xxlv)\n",
        "\n",
        "# MLP: MultiLayer Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do MLP\n",
        "m_mlp = MLPClassifier()\n",
        "\n",
        "# Alfa é um parâmetro para o termo de regularização, também conhecido como termo de penalidade,\n",
        "#    que combate o overfitting ao restringir o tamanho dos pesos.\n",
        "# Abaixo criamos um dicionário com valores que queremos testar para alpha\n",
        "m_params_mlp = {'alpha': [0.1, 0.2, 0.3, 0.4],\n",
        "               'max_iter': [1000, 1500, 2000, 2500]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "m_mlp_gs = GridSearchCV(m_mlp, m_params_mlp, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_mlp_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_mlp_best = m_mlp_gs.best_estimator_\n",
        "\n",
        "# mlp_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_mlp_scor = m_mlp_best.score(xm_valid, xxlv)\n",
        "\n",
        "# AB: AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador do ADA\n",
        "m_cl_ada = AdaBoostClassifier()\n",
        "\n",
        "# n_estimators int, padrão = 50\n",
        "# O número máximo de estimadores nos quais o aumento é finalizado. \n",
        "# Em caso de ajuste perfeito, o procedimento de aprendizado é interrompido mais cedo.\n",
        "m_params_ada = {'n_estimators': [30, 35, 40, 45, 50, 55, 65, 70]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores em n_estimators\n",
        "m_ada_gs = GridSearchCV(m_cl_ada, m_params_ada, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_ada_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_ada_best = m_ada_gs.best_estimator_\n",
        "\n",
        "# ada_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_ada_scor = m_ada_best.score(xm_valid, xxlv)\n",
        "\n",
        "# GNB: GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador\n",
        "m_gnb = GaussianNB()\n",
        "\n",
        "# Praticamente não há parâmetros para variar\n",
        "m_params_gnb = {'priors': [None]}\n",
        "\n",
        "# Usando o GridSearchCV \n",
        "m_gnb_gs = GridSearchCV(m_gnb, m_params_gnb, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_gnb_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_gnb_best = m_gnb_gs.best_estimator_\n",
        "\n",
        "# ada_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_gnb_scor = m_gnb_best.score(xm_valid, xxlv)\n",
        "\n",
        "# Classificador com vetores de suporte (SVC)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "\n",
        "m_cl_svc = SVC()\n",
        "\n",
        "# Vamos variar dois parâmetros dessa vez, o kernel e o parâmetro de regularização C\n",
        "\n",
        "m_params_svc = {'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1], 'kernel':['linear', 'sigmoid']}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "\n",
        "m_svc_gs = GridSearchCV(m_cl_svc, m_params_svc, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "\n",
        "m_svc_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "\n",
        "m_svc_best = m_svc_gs.best_estimator_\n",
        "\n",
        "# svc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_svc_scor = m_svc_best.score(xm_valid, xxlv)\n",
        "\n",
        "# Classificador com arvores de decisão (DTC)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "m_cl_dtc = DecisionTreeClassifier()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "m_params_dtc = {'max_depth':[3, 5, 7, 9, 11, 13, 15]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "m_dtc_gs = GridSearchCV(m_cl_dtc, m_params_dtc, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_dtc_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_dtc_best = m_dtc_gs.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_dtc_scor = m_dtc_best.score(xm_valid, xxlv)\n",
        "\n",
        "# Classificador por regressão linear\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um novo classificador SVC\n",
        "m_cl_LR = LinearRegression()\n",
        "\n",
        "# max_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado \n",
        "# nos dados de treinamento, também conhecido como overfitting.\n",
        "m_params_LR = {'normalize':[True, False]}\n",
        "\n",
        "# Usando o GridSearchCV para testar todos os valores dos parâmetros\n",
        "m_LR_gs = GridSearchCV(m_cl_LR, m_params_LR, cv=5)\n",
        "\n",
        "# Treinando o modelo\n",
        "m_LR_gs.fit(xm_treino, xxlt)\n",
        "\n",
        "# Salvando o melhor modelo\n",
        "m_LR_best = m_LR_gs.best_estimator_\n",
        "\n",
        "# dtc_scor servirá de base para verificar qual classificador \n",
        "# obteve o melhor score depois de ajustado\n",
        "m_LR_scor = m_LR_best.score(xm_valid, xxlv)\n",
        "# Score com o melhor modelo\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Regressão Logísica com parâmetros ajustados: \" +str(m_LogR_scor))\n",
        "print(\"Parâmetros ajustados: \"+str(m_LogR_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do KNN com parâmetro ajustado: \" +str(m_knn_scor))\n",
        "print(\"Parâmetro ajustado: \"+str(m_knn_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia da Floresta Aleatória com parâmetro ajustado: \" +str(m_rf_scor))\n",
        "print(\"Parâmetro ajustado: \"+str(m_rf_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do MLP com parâmetro ajustado: \" +str(m_mlp_scor))\n",
        "print(\"Parâmetro ajustado: \"+str(m_mlp_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do AdaBoost com parâmetro ajustado: \" +str(m_ada_scor))\n",
        "print(\"Parâmetros ajustado: \"+str(m_ada_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do Naive Bayes: \" +str(m_gnb_scor))\n",
        "print(\" Para esse classificador não ajustamos parâmetros.\")\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia do SVC com parâmetros ajustados: \" +str(m_svc_scor))\n",
        "print(\"Parâmetros ajustados: \"+str(m_svc_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia das árvores de decisão com parâmetro ajustado: \" +str(m_dtc_scor))\n",
        "print(\"Parâmetro ajustado: \"+str(m_dtc_gs.best_params_))\n",
        "print()\n",
        "print(\"*********************************************************************************\")\n",
        "print(\"Acurácia das árvores de decisão com parâmetro ajustado: \" +str(m_LR_scor))\n",
        "print(\"Parâmetro ajustado: \"+str(m_LR_gs.best_params_))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my5RChf6CTip"
      },
      "source": [
        "xm_nomes = ['Regressão Logística', 'KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', \n",
        "            'Naive Bayes', 'SVC', 'Árvores de Decisão', 'Regressão Linear']\n",
        "xm_scor = [m_LogR_scor, m_knn_scor, m_rf_scor, m_mlp_scor, m_ada_scor, m_gnb_scor, m_svc_scor, m_dtc_scor, m_LR_scor]\n",
        "mt_scor, mt_prim, mt_n_prim, mt_n_seg, mt_n_ter = rank_class(9, xm_nomes, xm_scor)\n",
        "xroda = mt_n_prim+mt_n_seg+mt_n_ter\n",
        "xxn_prim = mt_n_prim\n",
        "xxn_seg = mt_n_seg\n",
        "xxn_ter = mt_n_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores com mistura total\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xxn_prim != 0:\n",
        "    if xxn_prim == mt_n_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(mt_scor[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+mt_prim[q])\n",
        "      xxn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+mt_prim[q])\n",
        "      xxn_prim -= 1\n",
        "  else:\n",
        "    if xxn_seg != 0:\n",
        "      if xxn_seg == mt_n_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(mt_scor[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+mt_prim[q])\n",
        "        xxn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+mt_prim[q])\n",
        "        xxn_seg -= 1\n",
        "    else:\n",
        "      if xxn_ter != 0:\n",
        "        if xxn_ter == mt_n_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(mt_scor[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+mt_prim[q])\n",
        "          xxn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+mt_prim[q])\n",
        "          xxn_ter -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj8i3Nbc_bYx"
      },
      "source": [
        "#print(xabacate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIejvUmP5UTO"
      },
      "source": [
        "# Classificação com mistura seletiva\n",
        "import pandas as pd\n",
        "xmclassif = ['KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', 'Naive Bayes', 'SVC', 'Árvores de Decisão', 'Regressão Logística', 'Regressão Linear']\n",
        "clf_m = [m_knn_best,\n",
        "        m_rf_best,\n",
        "        m_mlp_best,\n",
        "        m_ada_best,\n",
        "        m_gnb_best,\n",
        "        m_svc_best,\n",
        "        m_dtc_best,\n",
        "        m_LogR_best,\n",
        "        m_LR_best]\n",
        "xmt_classif =[xxt_knn,\n",
        "             xxt_rf,\n",
        "             xxtreino_mlp,\n",
        "             xxt_ada,\n",
        "             xxt_svc,\n",
        "             xxt_gnb,\n",
        "             xxt_dtc,\n",
        "             xxt_LogR,\n",
        "             xxt_LR]\n",
        "xmv_classif =[xxv_knn,\n",
        "             xxv_rf,\n",
        "             xxv_mlp,\n",
        "             xxv_ada,\n",
        "             xxv_gnb,\n",
        "             xxv_svc,\n",
        "             xxv_dtc,\n",
        "             xxv_LogR,\n",
        "             xxv_LR]\n",
        "classif_m = []\n",
        "x=0\n",
        "clfm = []\n",
        "yhj = 0\n",
        "n_class_mt = mt_n_prim+mt_n_seg+mt_n_ter\n",
        "for x in range(n_class_mt):\n",
        "  for y in range(9):\n",
        "    if mt_prim[x] == xmclassif[y]:\n",
        "        classif_m.append(xmclassif[y])\n",
        "        clfm.append(clf_m[y])\n",
        "        yhj += 1\n",
        "m_xxt = pd.DataFrame(xxt)\n",
        "m_xxvd = pd.DataFrame(xxvd)\n",
        "sw_treino=[]\n",
        "sw_treino.append(m_xxt)\n",
        "sw_valid=[]\n",
        "sw_valid.append(m_xxvd)\n",
        "for n in range(xv1):\n",
        "  sw_treino.append(ptreino[n])\n",
        "  sw_valid.append(pval[n])\n",
        "xxm_treino = pd.concat(sw_treino, axis = 1)\n",
        "xxm_valid = pd.concat(sw_valid, axis = 1)\n",
        "i = 0\n",
        "xms_scores = []\n",
        "for name, clf in zip(classif_m, clfm):\n",
        "        clf.fit(xxm_treino, xxlt)\n",
        "        score = clf.score(xxm_valid, xxlv)\n",
        "        xms_scores.append(score)\n",
        "        i += 1\n",
        "ms_scor, ms_prim, ms_n_prim, ms_n_seg, ms_n_ter = rank_class(i, classif_m, xms_scores)\n",
        "xroda = ms_n_prim+ms_n_seg+ms_n_ter\n",
        "xxn_prim = ms_n_prim\n",
        "xxn_seg = ms_n_seg\n",
        "xxn_ter = ms_n_ter\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"************************************************************************\")\n",
        "print(\"Rankeamento dos Classificadores com mistura seletiva\")\n",
        "hq = 0\n",
        "jq = 0\n",
        "kq = 0\n",
        "for q in range(xroda):\n",
        "  if xxn_prim != 0:\n",
        "    if xxn_prim == ms_n_prim:\n",
        "      print(\"************************************************************************\")\n",
        "      print('Maior Acurácia: '+str(ms_scor[q]))\n",
        "      print('Classificador(es) com maior acurácia:')\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+ms_prim[q])\n",
        "      xxn_prim -= 1\n",
        "    else:\n",
        "      hq += 1\n",
        "      print(str(hq)+'. '+ms_prim[q])\n",
        "      xxn_prim -= 1\n",
        "  else:\n",
        "    if xxn_seg != 0:\n",
        "      if xxn_seg == ms_n_seg:\n",
        "        print(\"************************************************************************\")\n",
        "        print('Segunda Maior Acurácia: '+str(ms_scor[q]))\n",
        "        print('Classificador(es) com a segunda maior acurácia:')\n",
        "        jq += 1 \n",
        "        print(str(jq)+'. '+ms_prim[q])\n",
        "        xxn_seg -= 1\n",
        "      else:\n",
        "        jq += 1\n",
        "        print(str(jq)+'. '+ms_prim[q])\n",
        "        xxn_seg -= 1\n",
        "    else:\n",
        "      if xxn_ter != 0:\n",
        "        if xxn_ter == ms_n_ter:\n",
        "          print(\"************************************************************************\")\n",
        "          print('Terceira Maior Acurácia: '+str(ms_scor[q]))\n",
        "          print('Classificador(es) com a terceira maior acurácia:')\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+ms_prim[q])\n",
        "          xxn_ter -= 1\n",
        "        else:\n",
        "          kq += 1\n",
        "          print(str(kq)+'. '+ms_prim[q])\n",
        "          xxn_ter -= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laEQH8zH7rFq"
      },
      "source": [
        "# A função abaixo verifica se o arquivo já existe.\n",
        "def checkFileExistance(filePath):\n",
        "    try:\n",
        "        with open(filePath, 'r') as f:\n",
        "            return True\n",
        "    except FileNotFoundError as e:\n",
        "        return False\n",
        "    except IOError as e:\n",
        "        return False\n",
        "if checkFileExistance('/content/gdrive/My Drive/BancoDados/UIUC_1_num.dat'):\n",
        "    import sys\n",
        "    print('Você esqueceu de alterar o nome dos arquivos de Estatística.')\n",
        "    print('Deseja continuar (s/n) ?')\n",
        "    cvb = input()\n",
        "    if cvb == 'n':\n",
        "        file_pausa = input()\n",
        "        sys.exit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD6OrQjbutBr"
      },
      "source": [
        "# UIUC_1 : lista que contará os dados para estatística dos classificadores\n",
        "''' best_acc: melhor acurácia da CNN\n",
        "    xval : melhor acurácia dos classificadores \n",
        "           utilizando os vetores de recurso\n",
        "    xpp : nome dos classificadores que obtiveram\n",
        "          xval\n",
        "    n_prim, n_seg, n_ter : número de classificadores em xpp\n",
        "    xscor1: melhor curácia dos classificadores\n",
        "           utilizando os vetores de recurso\n",
        "           e com parâmetros ajustados\n",
        "    xprim : nome dos classificadores que obtiveram\n",
        "            xscor1\n",
        "    xn_prim, xn_seg, xn_ter : número de classificadores em xprim \n",
        "    et_scor : melhor acurácia dos classificadores \n",
        "             utilizando o empilhamento total\n",
        "    et_prim : nome dos classificadores que obtiveram\n",
        "              et_scor\n",
        "    et_n_prim, et_n_seg, et_n_ter : número de classificadores em et_prim\n",
        "    es_scor : melhor acurácia dos classificadores \n",
        "              utilizando o empilhamento seletivo\n",
        "    es_prim : nome dos classificadores que obtiveram\n",
        "              es_scor\n",
        "    es_n_prim, es_n_seg, es_n_ter : número de classificadores em et_prim\n",
        "    mt_scor : melhor acurácia dos classificadores \n",
        "              utilizando o mistura total\n",
        "    mt_prim : nome dos classificadores que obtiveram\n",
        "              mt_scor\n",
        "    mt_n_prim, mt_n_seg, mt_n_ter : número de classificadores em mt_prim\n",
        "    ms_scor : melhor acurácia dos classificadores \n",
        "              utilizando o mistura seletivo\n",
        "    ms_prim : nome dos classificadores que obtiveram\n",
        "              ms_scor\n",
        "    ms_n_prim, ms_n_seg, ms_n_ter : número de classificadores em ms_prim\n",
        "    xensemble : maior acurácia obtida na votação dos classificadores\n",
        "    Quando for resgatar os dados utilizar a seguinte indexação:\n",
        "    0.0 = 'Regressão Logística'\n",
        "    1.0 = 'KNN'\n",
        "    2.0 = 'Floresta Aleatória'\n",
        "    3.0 = 'MLP'\n",
        "    4.0 = 'AdaBoost'\n",
        "    5.0 = 'Naive Bayes'\n",
        "    6.0 = 'SVC'\n",
        "    7.0 = 'Árvores de Decisão'\n",
        "    8.0 = 'Regressão Linear', \n",
        "    9.0 = 'Processo Gaussiano'''\n",
        "as_1 = n_prim+n_seg+n_ter\n",
        "as_2 = xn_prim+xn_seg+xn_ter \n",
        "as_3 = et_n_prim+et_n_seg+et_n_ter\n",
        "as_4 = es_n_prim+es_n_seg+es_n_ter\n",
        "as_5 = mt_n_prim+mt_n_seg+mt_n_ter\n",
        "as_6 = ms_n_prim+ms_n_seg+ms_n_ter\n",
        "as_total = as_1 + as_2 + as_3 + as_4 + as_5 + as_6 + 2\n",
        "UIUC_1_num = np.zeros(20, dtype=np.float64)\n",
        "UIUC_1_num[0] = best_acc\n",
        "UIUC_1_num[1] = xensemb\n",
        "xvencedor=[]\n",
        "tg = 2\n",
        "# a seguir salvaremos os melhores scores obtidos\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = xval[z]\n",
        "  \n",
        "  tg += 1\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = xscor1[z]\n",
        "  \n",
        "  tg += 1\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = et_scor[z]\n",
        "  \n",
        "  tg += 1\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = es_scor[z]\n",
        "  \n",
        "  tg += 1\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = mt_scor[z]\n",
        "  \n",
        "  tg += 1\n",
        "for z in range(3):\n",
        "  UIUC_1_num[tg] = ms_scor[z]\n",
        "  tg += 1\n",
        "# A seguir salvaremos os classificadores que obtiveram os melhores scores\n",
        "for z in range(as_1):\n",
        "  \n",
        "  xvencedor.append(xpp[z])\n",
        "  \n",
        "for z in range(as_2):\n",
        "  \n",
        "  xvencedor.append(xprim[z])\n",
        "  \n",
        "for z in range(as_3):\n",
        "  \n",
        "  xvencedor.append(et_prim[z])\n",
        "  \n",
        "for z in range(as_4):\n",
        "  \n",
        "  xvencedor.append(es_prim[z])\n",
        "  \n",
        "for z in range(as_5):\n",
        "  \n",
        "  xvencedor.append(mt_prim[z])\n",
        "  \n",
        "for z in range(as_6):\n",
        "  \n",
        "  xvencedor.append(ms_prim[z])\n",
        "\n",
        "xnome_class = ['Regressão Logística', 'KNN', 'Floresta Aleatória', 'MLP', 'AdaBoost', \n",
        "                'Naive Bayes', 'SVC', 'Árvores de Decisão', 'Regressão Linear', 'Processo Gaussiano']\n",
        "bs_total = as_total - 2\n",
        "UIUC_1_class = np.empty(bs_total, dtype=np.float64)\n",
        "for i in range(bs_total):\n",
        "    if xvencedor[i] == xnome_class[0]:\n",
        "        UIUC_1_class[i] = float(0.0)\n",
        "    if xvencedor[i] == xnome_class[1]:\n",
        "        UIUC_1_class[i] = float(1.0)\n",
        "    if xvencedor[i] == xnome_class[2]:\n",
        "        UIUC_1_class[i] = float(2.0)\n",
        "    if xvencedor[i] == xnome_class[3]:\n",
        "        UIUC_1_class[i] = float(3.0)\n",
        "    if xvencedor[i] == xnome_class[4]:\n",
        "        UIUC_1_class[i] = float(4.0)\n",
        "    if xvencedor[i] == xnome_class[5]:\n",
        "        UIUC_1_class[i] = float(5.0)\n",
        "    if xvencedor[i] == xnome_class[6]:\n",
        "        UIUC_1_class[i] = float(6.0)\n",
        "    if xvencedor[i] == xnome_class[7]:\n",
        "        UIUC_1_class[i] = float(7.0)\n",
        "    if xvencedor[i] == xnome_class[8]:\n",
        "        UIUC_1_class[i] = float(8.0)\n",
        "    if xvencedor[i] == xnome_class[9]:\n",
        "        UIUC_1_class[i] = float(9.0)\n",
        "cs_total = as_total - 2\n",
        "UIUC_1_qtd = np.zeros(18, dtype=np.float64)\n",
        "UIUC_1_qtd[0]=n_prim\n",
        "UIUC_1_qtd[1]=n_seg\n",
        "UIUC_1_qtd[2]=n_ter\n",
        "UIUC_1_qtd[3]=xn_prim\n",
        "UIUC_1_qtd[4]=xn_seg\n",
        "UIUC_1_qtd[5]=xn_ter\n",
        "UIUC_1_qtd[6]=et_n_prim\n",
        "UIUC_1_qtd[7]=et_n_seg\n",
        "UIUC_1_qtd[8]=et_n_ter\n",
        "UIUC_1_qtd[9]=es_n_prim\n",
        "UIUC_1_qtd[10]=es_n_seg\n",
        "UIUC_1_qtd[11]=es_n_ter\n",
        "UIUC_1_qtd[12]=mt_n_prim\n",
        "UIUC_1_qtd[13]=mt_n_seg\n",
        "UIUC_1_qtd[14]=mt_n_ter\n",
        "UIUC_1_qtd[15]=ms_n_prim\n",
        "UIUC_1_qtd[16]=ms_n_seg\n",
        "UIUC_1_qtd[17]=ms_n_ter\n",
        "pprint(UIUC_1_num)\n",
        "pprint(UIUC_1_class)\n",
        "pprint(UIUC_1_qtd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq-N67fBiC7o"
      },
      "source": [
        "arq_est_num = \"/content/gdrive/My Drive/BancoDados/UIUC_1_num.dat\"\n",
        "arq_est_class = \"/content/gdrive/My Drive/BancoDados/UIUC_1_class.dat\"\n",
        "arq_est_qtd = \"/content/gdrive/My Drive/BancoDados/UIUC_1_qtd.dat\"\n",
        "UIUC_1_num.tofile(arq_est_num)\n",
        "UIUC_1_class.tofile(arq_est_class)\n",
        "UIUC_1_qtd.tofile(arq_est_qtd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiSK7d1GDhd"
      },
      "source": [
        "a_num = np.fromfile(arq_est_num)\n",
        "b_class = np.fromfile(arq_est_class)\n",
        "c_qtd = np.fromfile(arq_est_qtd)\n",
        "pprint(a_num == UIUC_1_num)\n",
        "pprint(b_class == UIUC_1_class)\n",
        "pprint(c_qtd == UIUC_1_qtd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiJQSR-0hTa"
      },
      "source": [
        "# Matriz de confusão para o classificadores utilizando os vetores de recurso\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 15, 15\n",
        "i = 0\n",
        "for k in names:\n",
        "    if names[i]=='Regressão Linear':\n",
        "        i += 1\n",
        "        continue\n",
        "    title_options = [(names[i]+\" utilizando Vetores de Recurso\", 'true')]\n",
        "    xrt='vertical'\n",
        "    for title, normalize in title_options:\n",
        "        disp = plot_confusion_matrix(w_classif[i], xxvd, xxlv,\n",
        "                                     display_labels=xclasses,\n",
        "                                     cmap = plt.cm.Blues,\n",
        "                                     xticks_rotation = xrt,       \n",
        "                                     normalize=normalize)\n",
        "    disp.ax_.set_title(title)\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "    plt.show()\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaI_Z6TyyKau"
      },
      "source": [
        "# Matriz de confusão para o classificadores, com parâmetros ajustados e utilizando os vetores de recurso\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 15, 15\n",
        "i = 0\n",
        "'''names = [\"0 KNN\", \n",
        "         \"1 SVC Linear\", \n",
        "         \"2 Processo Gaussiano\",\n",
        "         \"3 Árvores de Decisão\", \n",
        "         \"4 Floresta Aleatória\", \n",
        "         \"5 MLP\", \n",
        "         \"6 AdaBoost\",\n",
        "         \"7 Naive Bayes\", \n",
        "         \"8 Regressão Logística\", \n",
        "         \"9 Regressão Linear\"]'''\n",
        "for k in names:\n",
        "    if names[i]=='Regressão Linear':\n",
        "        continue\n",
        "    title_options = [(names[i]+\" com parâmetro(s) ajustado(s) e utilizando Vetores de Recurso\", 'true')]\n",
        "    xrt='vertical'\n",
        "    if i == 0:\n",
        "        e_classif = knn_best\n",
        "    if i == 1:\n",
        "        e_classif = svc_best\n",
        "    if i == 2:\n",
        "        e_classif = gpc_best\n",
        "    if i == 3:\n",
        "        e_classif = dtc_best\n",
        "    if i == 4:\n",
        "        e_classif = rf_best\n",
        "    if i == 5:\n",
        "        e_classif = mlp_best\n",
        "    if i == 6:\n",
        "        e_classif = ada_best\n",
        "    if i == 7:\n",
        "        e_classif = gnb_best\n",
        "    if i == 8:\n",
        "        e_classif = LogR_best\n",
        "    for title, normalize in title_options:\n",
        "        disp = plot_confusion_matrix(e_classif, xxvd, xxlv,\n",
        "                                     display_labels=xclasses,\n",
        "                                     cmap = plt.cm.Blues,\n",
        "                                     xticks_rotation = xrt,       \n",
        "                                     normalize=normalize)\n",
        "    disp.ax_.set_title(title)\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "    plt.show()\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}